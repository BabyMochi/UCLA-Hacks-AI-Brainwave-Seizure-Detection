# -*- coding: utf-8 -*-
"""RNN for Epilepsy Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oX4014svPfRmxvRf4KrW0OuM7JJAUT8l

# RNN for Epilepsy Classifier

This notebook is adapted from NeurotechX's Deep Learning Workshops, which can be found [here](https://github.com/NeuroTechX/dl-eeg-playground).

Epilepsy is the clinical term for recurrent seizures. A seizure occurs when there are abnormal, excessive, and hypersynchronous neuron firings that lead to a temporary disruption of the brain. In this notebook, we will build and train a recurrent neural network to classify epilepsy data from the [Bonn University Epilepsy Dataset](http://epileptologie-bonn.de/cms/front_content.php?idcat=193&lang=3). This dataset consists of 5 sets of data, each with 100 EEG recordings. The labels for each set is as follows:

**Set A**: Non-epileptic, Eyes open

**Set B**: Non-epileptic, Eyes closed

**Set C**: Epileptic, Interictal (in between seizures)

**Set D**: Epileptic, Interictal

**Set E**: Epileptic, Ictal (during a seizure)

For this demo, we will attempt to classify non-epileptic, eyes open data vs epileptic, ictal data (Set A vs Set E). Feel free to experiment with different combinations of datasets and see which classification tasks are suitable for this model (and for other models).

## Importing the Data

First, we must download the dataset and store it somewhere. Previously, we've done this by hand and stored it in Google Drive. This time, we're using ```wget```, a command-line utility used to retrieve files from web servers. Note that we must preface these commands with a ```!``` because these are commands, not Python code.
"""

"""
Retrieve the zipped datafiles and unzip them into directories.
Uncomment the lines for the sets you wish to use.
"""

!wget https://repositori.upf.edu/bitstream/handle/10230/42894/Z.zip  # Set A
# !wget https://repositori.upf.edu/bitstream/handle/10230/42894/O.zip  # Set B
# !wget https://repositori.upf.edu/bitstream/handle/10230/42894/N.zip  # Set C
# !wget https://repositori.upf.edu/bitstream/handle/10230/42894/F.zip  # Set D
!wget https://repositori.upf.edu/bitstream/handle/10230/42894/S.zip  # Set E

# -d allows you to create a directory to save the contents of the unzipped file
!unzip Z.zip -d 'Set A' 
# !unzip O.zip -d 'Set B'
# !unzip N.zip -d 'Set C'
# !unzip F.zip -d 'Set D'
!unzip S.zip -d 'Set E'

"""Next, we import the libraries needed to store and visualize this data. ```numpy``` and ```matplotlib.pyplot``` are fairly obvious; we've used them countless times to work with EEG data in previous workshops. The ```os``` library will be used to iterate through all the datafiles so that we can load them into an list. The ```tqdm``` library is optional; it's used to display loading bars that show the progress of a loop since loading all this data may take a while."""

import numpy as np
import matplotlib.pyplot as plt  
import os
from tqdm import tqdm

"""Finally, we load our data into the program. The data is stored as a giant list, which consists of tuples of the form ```(data, label)```."""

# Set the directory names
DATA_DIR_A = './Set A/'
# DATA_DIR_B = './Set B/'
# DATA_DIR_C = './Set C/'
# DATA_DIR_D = './Set D/'
DATA_DIR_E = './Set E/'

# Assign numerical labels to each class of data. 
# In our case, we have two possible labels (A vs E).
LABEL_A = 0
LABEL_E = 1

"""
Returns a list of all of our data. Each element in the 
list is a tuple consisting of an EEG data recording (4096 points) 
and its associated label (Set A or Set E).
"""
def load_data():
    labeled_data = []

    for fname in tqdm(os.listdir(DATA_DIR_A)):
        data = np.loadtxt(DATA_DIR_A + fname)
        labeled_data.append([np.array(data), LABEL_A])
        
    for fname in tqdm(os.listdir(DATA_DIR_E)):
        data = np.loadtxt(DATA_DIR_E + fname)
        labeled_data.append([np.array(data), LABEL_E])
        
    return labeled_data

all_data = load_data()

print(f"There are {len(all_data)} files.")
print(all_data[0])

"""## Visualizing the Data

To gain a better understanding of how the data is arranged, we will plot a few of the EEG signals.
"""

"""
Randomly sample four EEG signals from each set
and plot them.
"""
import random

plt.figure(figsize=(20, 10))

# Samples from Set A
samples_a = [] 
while len(samples_a) < 4:
  curr_idx = random.randint(0, len(all_data))
  if (all_data[curr_idx][1] == LABEL_A):
    samples_a.append(all_data[curr_idx][0])

# Samples from Set E
samples_e = []
while len(samples_e) < 4:
  curr_idx = random.randint(0, len(all_data))
  if (all_data[curr_idx][1] == LABEL_E):
    samples_e.append(all_data[curr_idx][0])

for i in range(0, 4):
  plt.subplot(4,2, 1 + i*2)
  plt.plot(samples_a[i])
  plt.subplot(4,2,2 + i*2)
  plt.plot(samples_e[i])

plt.suptitle('Class A vs Class E', fontsize=20)
plt.show()

"""## Preparing the Data

Before we can feed our data into the neural network, there are a few steps we must take.

First, we divide our data into training data and test data. The training data, as the name implies, is used when training the model (i.e. adjusting the weights). The test data will be used to evaluate the performance of our model at the end.

Second, we have to shape our data into a form that will be accepted by our network. ```X_train``` and ```X_test``` represent the independent variables (i.e. the datapoints of each EEG recording), while ```Y_train``` and ```Y_test``` represent the dependent variables (i.e. the label/expected value for each recording). 
"""

from sklearn.utils import shuffle
shuffled_data = shuffle(all_data)

# Use 80% of the data to be used as training data.
nb_train = round(len(shuffled_data) * 0.8)
data_train = shuffled_data[0:nb_train]
data_test = shuffled_data[nb_train:]

# Split the data into X (input) and Y (expected output)
# and stack it as a single array
X_train = np.array([d[0] for d in data_train])
Y_train = np.array([d[1] for d in data_train])

X_test = np.array([d[0] for d in data_test])
Y_test = np.array([d[1] for d in data_test])

print(X_train.shape)
print(Y_train.shape)

# Add an extra dimension to convert each sample into an 
# array that can be fed into the neural network
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
Y_train = Y_train.reshape(Y_train.shape[0], 1)

X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
Y_test = Y_test.reshape(Y_test.shape[0], 1)

"""## Building the Model

Finally, we are able to build our neural network. Here, we are using ```keras```, a high level API that allows for quick development of neural networks. The ```Sequential``` model lets us add layers incrementally to design the model and then create it using the ```compile``` method. 

Feel free to play around with the layers themselves as well as the parameters that are associated with them, and be sure to check out the [Keras API Reference](https://keras.io/api/) for more details on the different layers you can add.
"""

from keras.models import Sequential
from keras.layers import Input, Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import LSTM

hidden_size = 64

# Initialize the model
model = Sequential()

# Specify the input to the model 
model.add(Input(shape=(X_train.shape[1], 1)))

# Add a Long Short-Term Memory Layer (form of RNN) with 64 neurons
model.add(LSTM(hidden_size))

# Add a Dropout Layer, which randomly sets inputs to 0 35% of the time
# This is a regularization technique commonly used to prevent overfitting
model.add(Dropout(0.35))

# Add a Dense Layer, which is a layer of neurons in a neural network that 
# receive inputs from all neurons from the previous layer. 
# In this case, this is our output layer.
model.add(Dense(1))

# Adds an Activation Layer, which simply provides an activation function
model.add(Activation('sigmoid'))

# Compile the model, specifying the loss function and optimizer to use
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['mae', 'acc'])

print(model.summary())

"""## Training the Model

Now that the model is compiled, we can train it and evaluate its performance on our test data. Training is done using the ```model.fit()``` method. Note that we have three parameters following ```X_train``` and ```Y_train```. 


1.   ```epochs``` specifies the number of times we iterate through the entire set of X and Y values.
2.   ```batch_size``` is used to feed in chunks of our data at a time to speed up training.
2.   ```validation_split``` splits up our training data further into training data and validation data. Validation data is used to evaluate the model after every epoch, almost like an intermediary training set.

Note: change your runtime to make use of GPU (```Runtime``` > ```Change runtime type``` > ```GPU```) to speed up the process of training (otherwise it will probably take forever to run).
"""

batch_size = 16
nb_epoch = 20

history = model.fit(X_train, Y_train, validation_split=0.2, batch_size=batch_size, epochs=nb_epoch)

"""```model.fit()``` returns a history object, which we can use to see the history of training loss and metrics to evaluate the training process."""

# list all data in history
print(history.history.keys())

# summarize history for accuracy
plt.plot(history.history['acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'], loc='upper left')
plt.show()

"""## Evaluating the Model

Now that we've analyzed the performance of the model during training, it is time to evaluate using our test data.
"""

score = model.evaluate(X_test, Y_test, batch_size=batch_size)

"""Based on the results of ```model.evaluate``` above, we can see that our model had a fairly high accuracy for our test data. """

results = model.predict(X_test)
print(results)

"""```model.predict``` is used to classify new data, not just the test data. In this case, we get numerical results between 0 and 1. Values closer to 0 represent class A and values closer to 1 represent class E.

## Conclusion

As we can see, our model performs quite well in classifying between Sets A and E. Try evaluating the model on data from different sets to see if our model still performs well. Try training a different classifier using different models and parameters. Experiment! ```keras``` and Google Colab make it very easy to quickly prototype neural networks for all sorts of tasks.
"""